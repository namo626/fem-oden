\documentclass[12pt,notitlepage]{extreport}
\usepackage{amsmath}
\usepackage{eufrak}
\usepackage[mathscr]{euscript}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage[margin=1.2in]{geometry}
\usepackage{libertine}
\usepackage{setspace}
\usepackage{newtxmath}
\usepackage[T1]{fontenc}
\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage[skip=5pt plus1pt, indent=20pt]{parskip}

\singlespacing

\author{Chayanon Wichitrnithed}
\title{Parallel 2D Finite Elements with MPI}

\begin{document}
\maketitle

\begin{abstract}
  We code a parallel version of the standard Galerkin finite element method using MPI and PETSc. The test problem is a 2D Poisson equation with essential boundary conditions. We outline both the theory and implementation\footnote{https://github.com/namo626/fem-oden} of the method and detail how parallelism is added. In particular, we parallelize 1) reading of the input, 2) calculation of the global matrices, and 3) solving of the linear system (using PETSc). Timing measurements show that we gain significant speedup from 2) while the rest do not scale as well. This suggests that efforts should be put in the parallelization on 3) in order to achieve a good overall speedup in a more complex finite element program.
\end{abstract}
\section*{Introduction}
The finite element method (FEM) is a class of numerical methods for solving partial differential equations (PDE) which have originally been used to solve problems in engineering such as solid mechanics and structural analysis. However, advancements in its mathematical theory have made FEM widely applicable to many scientific areas such as electromagnetism and fluid dynamics. One major example is the widely used Advanced Circulation Model (ADCIRC) which simulates hurricane storm surges by solving hyperbolic PDEs using FEM.

The main idea behind the method is that, instead of approximating with derivatives at points (as in finite difference methods), it approximates the whole solution by combining many simple \textit{basis functions} over the domain. Morever, the domain itself is divided into smaller parts called \textit{finite elements} to facilitate the computation of these functions. Consequently the formulation and implementation of the method are very different from traditional finite difference methods. In this work we will implement a simple FEM solver in two dimensions and measure the performance gained from parallelizing various parts of the program.

For simplicity, the PDE we are solving will be the Poisson equation with prescribed boundary values (Dirichlet boundary condition)
\begin{align}
  - \Delta u &= f(x,y) \quad \text{on} \quad \Omega \\
  u &= g(x,y) \quad \text{on} \quad \partial \Omega,
\end{align}
where $f : \Omega \rightarrow \mathbb{R}$, $g: \partial\Omega \rightarrow \mathbb{R}$ are given functions.

\subsection*{Weak formulation}
The equations above are called the \textit{strong form} of the PDE. To solve it using FEM we will convert them to the \textit{weak form} as follows.

Integrating (1) over the domain gives
\begin{align}
  -\int_\Omega v \Delta u = \int_\Omega v f
\end{align}
and using integration by parts (Green's identity) gives
\begin{align}
  \int_\Omega \nabla v \cdot \nabla u = \int_\Omega v f.
\end{align}
Notice now that the second derivative constraint for $u$ has been relaxed. The weak formulation can now be stated:

\textit{Find $u \in S$ such that (4) is satisfied for every $v \in V$},

\noindent where $S$ and $V$ are appropriate function spaces.

\subsection*{Galerkin approximation}
As it stands, the weak formulation above is exact. However, the function spaces can be infinite-dimensional, so we will have to approximate them with finite ones. That is, we can search for an approximate solution $u_h$ in a finite-dimensional vector space $V_h \subset V$ and write it as
\begin{align}
  u(x) \approx u_h(x) = \sum_{i=1}^d u_i \phi_i(x)
\end{align}
where $d$ is the dimension of $V_h$ and each $\phi_i$ is a basis function (vector) of $V_h$. In our 2D case, one such basis function is shown in Figure \ref{fig:basis} below.
\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{basis.png}

  \caption{A ``pyramid'' basis function built from linear shape functions on triangular elements}
  \label{fig:basis}
\end{figure}
If we substitute this representation into the weak form, we get a system of linear equations:
\begin{align}
  K\hat{u} = F
\end{align}
where
\begin{align}
  K_{ij} &= \int_\Omega (\phi_{j,x} \phi_{i,x} + \phi_{j,y} \phi_{i,y}) \\
  F_i &= \int_\Omega f \phi_i \\
  \hat{u}_i &= u_i.
\end{align}
Once we have solved for the coefficients $u_i$, we can use them to construct the approximate solution in (5).

However, actual computation requires further steps. We break the integrals into sums of smaller integrals on each element $\Omega^e$:
\begin{align}
  K_{ij} &= \sum_e K^e_{ij} = \sum_e \int_{\Omega^e} (\phi_{j,x} \phi_{i,x} + \phi_{j,y} \phi_{i,y}) \\
  F_i &= \sum_e F^e_i = \sum_e \int_{\Omega^e} f \phi_i
\end{align}
Thus, the work on each component of $K$ and $F$ can be split among multiple elements. Morever, we avoid completely calculating the integrals on each unique $\Omega^e$ by performing a change of basis to a \textit{reference triangle} $\hat{\Omega}^e$, so the computations would only depend on each element's vertices.

We can now lay down the main tasks required to use this method:
\begin{enumerate}
\item Read in the domain information. This includes the number of basis functions (nodes), their locations in the domain, and the connectivity for each element.
\item For each element $e$, compute its local information such as the change-of-basis map and quadrature values, and perform numerical integration to obtain the local matrices $K^e$ and $F^e$.
\item Sum all the elemental contributions into the global matrices $K$ and $F$, and solve the resulting linear system.
\end{enumerate}
Specifically, we will parallelize steps 1 and 2. We will use a dedicated external library PETSc to parallelize parts of step 3, since numerical linear algebra is a very specialized field that requires significant domain knowledge. The implementation details are explained in the following section.

\section*{Implementation}
We use a square domain with triangular elements and linear basis functions, so each element has 3 nodes. An example of the discretized domain is shown in Figure \ref{fig:mesh}. The program will accept 2 input files: a list of $x$ and $y$ coordinates for every node, and a list of nodes for each element. Each line of each file can be viewed as $[x, y]$ and $[node1, node2, node3]$, respectively.
\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{mesh.png}
  \caption{A finite element mesh with 8 triangular elements per side}
  \label{fig:mesh}
\end{figure}

The most natural parallelization strategy is for the elements to be split among processors. Each element is constructed as a C++ object with local data and methods to compute its local matrices and to assemble them into the global matrices. Parallelization is done in MPI, and we use PetSC to handle global matrix calculations using the same MPI communicator as the other parts of the program.

\subsection*{Reading of Input Data}
 Since every processor has to be able to access the node list, we let rank 0 read in the node list and broadcast this information to the other ranks.
\begin{algorithm}[htp]
  \caption{Reading the node list}
  \begin{algorithmic}
    \Procedure{readNodes}{\textit{file}} \Comment{Output: Number of nodes, coordinate of each node}
    \State \textit{nodesCount := 0}
    \State Initialize \textit{nodesCoords}[] vector
    \If{rank = 0}
    \For{each line in \textit{file}}
    \State Read $x$ and $y$ coordinates into \textit{nodesCoords}[]
    \State \textit{nodesCount++}
    \EndFor
    \EndIf
    \State MPI\_Bcast \textit{nodesCount}
    \State Resize \textit{nodesCoords}[] to be of size \textit{nodesCount}
    \State MPI\_Bcast \textit{nodesCoords}[]
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

Reading in the elements, however, can be done in parallel since each rank only needs its local subset of all elements. We use MPI-IO to define an offset based on each rank to read the file in parallel, so each rank can retrieve its elements without having to communicate with other ranks.
\begin{algorithm}[htp]
  \caption{Reading the element list}
  \begin{algorithmic}
    \Procedure{readElems}{\textit{file}}
    \State Get \textit{numlines} from MPI\_File\_get\_size
    \State Initialize read buffer \textit{buf}
    \State \textit{offset} := \textit{sizeof(int)} * \textit{rank} * 3 * \textit{numlines} / number of ranks
    \State MPI\_File\_Open \textit{file}
    \State MPI\_Seek using \textit{offset}
    \State MPI\_Read the local element list into \textit{buf}
    \State Construct the local list of Element objects from \textit{buf}
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\subsection*{Computing and assembling the matrices}
Once each rank obtains its local list of elements, it can proceed to compute the local quantities independently. We construct the global matrices $K$ and $F$ using PETSC collectives which store them in parallel across MPI\_COMM\_WORLD.

Inside the assemble() method, we use the special handle MatSetValue() and VecSetValue() to mark the locations in the global matrices that the element will add to. We then start the communication using VecAssemblyBegin() and MatAssemblyBegin() which asynchronously transfer data to those marked locations. Similar to using asynchronous Isend/Irecv, we have to wait for the transfers to be completed before proceeding.

\begin{algorithm}[htp]
  \caption{Assembly method of each element}
  \begin{algorithmic}
    \Procedure{Element.assemble}{$K$, $F$}
    \For{each component $k$ in $K^e$}
    \If{$k$ is not the prescribed boundary}
    \State MatSetValues(\ldots, ADD\_VALUES) to the appropriate location in $K$
    \EndIf
    \EndFor

    \For{each component $f$ in $F^e$}
    \If{$k$ is not the prescribed boundary node}
    \State VecSetValues(\ldots, ADD\_VALUES) to the appropriate location in $F$
    \EndIf
    \EndFor
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[htp]
  \caption{Assembling the global matrices}
  \begin{algorithmic}
    \State VecCreateMPI(MPI\_COMM\_WORLD, \ldots, $F$)
    \State MatCreateMPI(MPI\_COMM\_WORLD, \ldots, $K$) \\
    \For{each element $e$ in the local element list}
    \State $e$.precompute() \Comment{Compute the local basis functions}
    \State $e$.computeKF() \Comment{Compute local matrices $K^e$ and $F^e$}
    \State $e$.assemble($K$, $F$)
    \EndFor
    \State VecAssemblyBegin($F$);
    \State MatAssemblyBegin($K$,MAT\_FINAL\_ASSEMBLY);

    \State VecAssemblyEnd($F$);
    \State MatAssemblyEnd($K$,MAT\_FINAL\_ASSEMBLY);
  \end{algorithmic}
\end{algorithm}

\subsection*{Solving the linear system}
We let PETSc handle the solving the (sparse) system $K\hat{u} = F$ in parallel using the KSP solver package. The resulting solution vectors are then gathered into rank 0 and then written sequentially to a file. The format of the output is that each line contains the $x$ and $y$ coordinates, followed by the corresponding value $u_h(x,y)$.

\section*{Results}
The test problem is
\begin{align}
  -\Delta u(x,y) &= \frac{4}{81}e^{-(x^2/9+y^2/9)}(x^2 + y^2 - 9) \quad \text{on} \quad (-5,5) \times (-5,5) \\
  u(x,y) &= e^{-(x^2/9 + y^2/9)} \quad \text{on the boundary}.
\end{align}
The exact solution is
\begin{align}
  u(x,y) = e^{-(x^2/9 + y^2/9)}.
\end{align}
We plot a sample solution with a  16x16 mesh below in Figure \ref{fig:plot}.

\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{plot1.png}

  \caption{FEM solution on a 16x16 mesh (left) and the exact solution (right)}
  \label{fig:plot}
\end{figure}
Next we plot the parallel scaling of the program. We fix the mesh size to be 1024x1024 while increasing the number of processors from 2 to 32. The plots in Figure \ref{fig:speedup} show the speedup for different parallel sections of the program.
\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{speedup.png}

  \caption{Parallel speedup for different parts of the program. Dashed lines represent ideal speedup.}
  \label{fig:speedup}
\end{figure}

The first thing to notice is that all parts see some speedup except for reading the node list. Indeed, there isn't really any ``parallelism'' in reading the node list because it merely reads in the list sequentially and broadcasts it to all the other ranks. As there are more processors, broadcasting takes more time and so the speedup is negative as shown here.

Reading the element list, on the other hand, has close to linear speedup in the beginning then diminishes after 16 cores. This is a little surprising because there seems to be no apparent communication between the ranks that we explicitly code, but this could perhaps be attributed to IO issues in MPI-IO. Another surprising result is that the scaling in assembling the global matrices is quite good; it remains linear all the way and is close to the ideal speedup. We speculate this to two reasons. One, PETSc may be using a reduction-like operation when adding values to the global matrices, in which case the runtime is better than linear. Second, there may not be that many components from the ranks that overlap in the global matrices, in which case there is no race condition and the sequential work is reduced.

For solving the linear system, we see dimishing speedup quite early. The KSP default solver appears to use the Block Jacobi method, but as mentioned, numerical linear algebra is beyond the scope of this work and will not be analyzed here.

\section*{Summary}
We have implemented a basic parallel finite element solver in MPI and tested the scaling on different parts of the program. While some speedup has been gained, the overall scaling diminishes quite quickly as more processors are used. The most significant speedup is gained from parallelizing the assembling of the global matrices, which appears close to linear and is maintained even at higher number of cores. This is significant because in a bigger finite element program, one adds more complexity to the element calculations such as higher-order basis functions, higher-order quadrature rules, or  even timestepping for time-dependent problems. Thus the assembly and linear system solving are the two that constitute a larger part of the program than input reading which is only done once. For a more complex FEM solver, then, the focus should be on optimizing the components of the global matrices and choosing the best linear solver that achives the best parallelism.

\begin{thebibliography}{3}
\bibitem{texbook}
Becker, E. B., Carey, G. F., Oden, J. T. (1981) \emph{Finite Elements, Volume 1}, Prentice Hall.

\bibitem{lamport94}
Hughes, T. J. (1994) \emph{The Finite Element Method: Linear Static and Dynamic Finite Element Analysis}, Prentice Hall.
Wesley, Massachusetts, 2nd ed.
\end{thebibliography}

\end{document}
